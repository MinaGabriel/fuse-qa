{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f34f254",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistralai_Mistral-Nemo-Instruct-2407-FUSEQA_20260218-1314\n",
      "GPUs: 1\n",
      "Logged in to Hugging Face.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import tqdm\n",
    "import json\n",
    "from fuseqa import *\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import login\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_error()\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
    "# runr> export HF_TOKEN=your_token_here\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "#MODEL_NAME = \"meta-llama/Meta-Llama-3-8B\"\n",
    "#MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "MODEL_NAME = \"mistralai/Mistral-Nemo-Instruct-2407\"\n",
    "\n",
    "TOP_K = 10\n",
    "\n",
    "\n",
    "# Types: \n",
    "#   - PARAMETRIC:\n",
    "#   - FUSEQA:\n",
    "#   - FUSEQA-SRE:  \n",
    "RUN_TYPE = \"FUSEQA\"\n",
    "USE_CONTEXT = RUN_TYPE in (\"FUSEQA\", \"FUSEQA-SRE\")\n",
    "print(hf_model_to_filename(MODEL_NAME + \"-\" + RUN_TYPE))\n",
    "print(\"GPUs:\", torch.cuda.device_count())\n",
    "\n",
    "token = os.getenv(\"HF_TOKEN\")\n",
    "if token:\n",
    "    login(token=token)\n",
    "    print(\"Logged in to Hugging Face.\")\n",
    "else:\n",
    "    print(\"HF_TOKEN not set.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6730b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a230238d62874d80b883fe95f8ee24ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/622 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda1d6d86432453a8e282df78ee78c6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca20b2c63ec439591397fb84f3d1f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50079ec9eb534bebb0b84d3aeab4e751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86dfde49ba349c2bcaa58c35deb871d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb910361019c4b909915b6b97f36f7c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd0cbb2c396b4287a6eb95ca744a0c25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"Model device:\", next(model.parameters()).device)\n",
    "\n",
    "if hasattr(model, \"hf_device_map\"):\n",
    "    print(\"Device map:\", model.hf_device_map)\n",
    "else:\n",
    "    print(\"Single-device load (no map needed)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070af6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "from datasets import load_dataset\n",
    "# ds = load_dataset(\"MinaGabriel/popqa-with-retrieval-20\")[\"test\"].select(range(25))\n",
    "ds = load_dataset(\"MinaGabriel/popqa-with-retrieval-20\")[\"test\"]\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a17d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = [\"ALL\", \"LONG-TAIL\", \"INFREQUENT\", \"FREQUENT\"]\n",
    "\n",
    "counts  = {g: 0 for g in groups}\n",
    "em_hits = {g: 0 for g in groups}\n",
    "\n",
    "\n",
    "def update_metrics(tier, em):\n",
    "    for grp in (\"ALL\", tier):\n",
    "        counts[grp]  += 1\n",
    "        em_hits[grp] += em\n",
    "\n",
    "\n",
    "def current_scores():\n",
    "    return {\n",
    "        \"ALL_EM\":     safe_div(em_hits[\"ALL\"],        counts[\"ALL\"]),\n",
    "        \"Long_Tail\":  safe_div(em_hits[\"LONG-TAIL\"],  counts[\"LONG-TAIL\"]),\n",
    "        \"Infrequent\": safe_div(em_hits[\"INFREQUENT\"], counts[\"INFREQUENT\"]),\n",
    "        \"Frequent\":   safe_div(em_hits[\"FREQUENT\"],   counts[\"FREQUENT\"]),\n",
    "    }\n",
    "\n",
    "\n",
    "progress_bar = tqdm.tqdm(\n",
    "    total=len(ds[\"question\"]),\n",
    "    desc=\"Generating + Evaluating\",\n",
    "    dynamic_ncols=True\n",
    ")\n",
    "\n",
    "\n",
    "outfile = hf_model_to_filename(MODEL_NAME + \"-\" + RUN_TYPE) + \".jsonl\"\n",
    "\n",
    "with open(outfile, \"w\", encoding=\"utf-8\", buffering=1) as f:\n",
    "\n",
    "    num_examples = len(ds[\"question\"])\n",
    "\n",
    "    for i in range(num_examples):\n",
    "\n",
    "        ex = {k: ds[k][i] for k in ds.column_names}\n",
    "\n",
    "        q      = ex[\"question\"]\n",
    "        s_pop  = int(ex.get(\"s_pop\", 0))\n",
    "        tier   = tier_from_spop(s_pop)\n",
    "\n",
    "        gold            = parse_list(ex.get(\"possible_answers\"))\n",
    "        gold_norm_set   = {norm(g) for g in gold if norm(g)}\n",
    "\n",
    "        retrieved = ex.get(\"retrieved_docs\") or []\n",
    "        context   = build_context(retrieved, k=10) if USE_CONTEXT else \"\"\n",
    "\n",
    "        pred = ask_llm_generate(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            q,\n",
    "            context,\n",
    "            use_context=USE_CONTEXT,\n",
    "            llama_device=\"cuda:0\"\n",
    "        )\n",
    "\n",
    "        pred_norm = norm(pred)\n",
    "        em        = int(pred_norm in gold_norm_set) if gold_norm_set else 0\n",
    "\n",
    "        update_metrics(tier, em)\n",
    "\n",
    "        record = {\n",
    "            \"i\":        i,\n",
    "            \"s_pop\":    s_pop,\n",
    "            \"tier\":     tier,\n",
    "            \"question\": q,\n",
    "            \"gold\":     gold,\n",
    "            \"pred\":     pred,\n",
    "            \"em\":       em,\n",
    "        }\n",
    "\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "        progress_bar.update(1)\n",
    "\n",
    "        scores = current_scores()\n",
    "        progress_bar.set_postfix({k: f\"{v:.4f}\" for k, v in scores.items()})\n",
    "\n",
    "\n",
    "progress_bar.close()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# REPORT\n",
    "# =========================\n",
    "\n",
    "lines = []\n",
    "\n",
    "lines.append(\"=\" * 80)\n",
    "lines.append(\"PopQA Exact Match Report â€” Tiers (Long-Tail / Infrequent / Frequent)\")\n",
    "lines.append(\"=\" * 80)\n",
    "\n",
    "lines.append(f\"Model: {MODEL_NAME}\")\n",
    "lines.append(f\"Mode: {RUN_TYPE}\")\n",
    "lines.append(f\"Run Time: {now}\")\n",
    "lines.append(f\"Slice: [0,{len(ds)}) | n={len(ds)}\")\n",
    "\n",
    "lines.append(\"\")\n",
    "lines.append(\"Tier Definitions (by s_pop):\")\n",
    "lines.append(\"- LONG-TAIL:  s_pop < 100\")\n",
    "lines.append(\"- INFREQUENT: 100 <= s_pop < 10000\")\n",
    "lines.append(\"- FREQUENT:   s_pop >= 10000\")\n",
    "\n",
    "lines.append(\"\")\n",
    "lines.append(\"Exact Match (EM):\")\n",
    "\n",
    "for name in groups:\n",
    "    n = counts[name]\n",
    "    lines.append(f\"- {name:<10} n={n:<6} EM={safe_div(em_hits[name], n):.4f}\")\n",
    "\n",
    "\n",
    "report_file = hf_model_to_filename(MODEL_NAME + \"-\" + RUN_TYPE) + \".report.txt\"\n",
    "\n",
    "with open(report_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(lines))\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"Saved report: {report_file} | \"\n",
    "    f\"LT EM={safe_div(em_hits['LONG-TAIL'], counts['LONG-TAIL']):.4f} | \"\n",
    "    f\"INF EM={safe_div(em_hits['INFREQUENT'], counts['INFREQUENT']):.4f} | \"\n",
    "    f\"FREQ EM={safe_div(em_hits['FREQUENT'], counts['FREQUENT']):.4f}\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
