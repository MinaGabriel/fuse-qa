{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae0fd834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! git clone https://github.com/MinaGabriel/fuse-qa.git\n",
    "# % cd /content/fuse-qa\n",
    "# ! export HF_TOKEN=\n",
    "# ! pip install -e . \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37a8cb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama_Meta-Llama-3-8B-FUSEQA_20260220-1050\n",
      "GPUs: 1\n",
      "HF login: done\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dddd58d0d1b04011b5e6c10b099b9c4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import logging\n",
    "from huggingface_hub import login\n",
    "\n",
    "from fuseqa import *\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Setup\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B\"\n",
    "# MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "# MODEL_NAME = \"google/gemma-2-2b-it\"\n",
    "# MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "# MODEL_NAME = \"openai/gpt-oss-20b\"\n",
    "RUN_TYPE = \"FUSEQA\"\n",
    "USE_CONTEXT = RUN_TYPE in (\"FUSEQA\", \"FUSEQA-SRE\")\n",
    "\n",
    "print(hf_model_to_filename(MODEL_NAME + \"-\" + RUN_TYPE))\n",
    "print(\"GPUs:\", torch.cuda.device_count())\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# HuggingFace Auth (optional)\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "if (token := os.getenv(\"HF_TOKEN\")):\n",
    "    login(token=token)\n",
    "    print(\"HF login: done\")\n",
    "else:\n",
    "    print(\"HF login: skipped\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Load Model + Tokenizer\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Device Info\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "print(\"Model device:\", device)\n",
    "\n",
    "if hasattr(model, \"hf_device_map\"):\n",
    "    print(\"Device map:\", model.hf_device_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "070af6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"MinaGabriel/popqa-with-retrieval-20\")[\"test\"].select(range(125))\n",
    "#ds = load_dataset(\"MinaGabriel/popqa-with-retrieval-20\")[\"test\"]\n",
    "len(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d8b3e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REPORTS:\n",
    "import os\n",
    "import tqdm\n",
    "from contextlib import nullcontext\n",
    "\n",
    "counts  = {g: 0 for g in (\"ALL\", \"LONG-TAIL\", \"INFREQUENT\", \"FREQUENT\")}\n",
    "em_hits = {g: 0 for g in (\"ALL\", \"LONG-TAIL\", \"INFREQUENT\", \"FREQUENT\")}\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "def update_metrics(tier, em):\n",
    "    counts[\"ALL\"] += 1\n",
    "    em_hits[\"ALL\"] += em\n",
    "\n",
    "    if tier in counts:\n",
    "        counts[tier] += 1\n",
    "        em_hits[tier] += em\n",
    "\n",
    "\n",
    "def current_scores():\n",
    "    return {\n",
    "        \"ALL_EM\":     safe_div(em_hits[\"ALL\"],        counts[\"ALL\"]),\n",
    "        \"Long_Tail\":  safe_div(em_hits[\"LONG-TAIL\"],  counts[\"LONG-TAIL\"]),\n",
    "        \"Infrequent\": safe_div(em_hits[\"INFREQUENT\"], counts[\"INFREQUENT\"]),\n",
    "        \"Frequent\":   safe_div(em_hits[\"FREQUENT\"],   counts[\"FREQUENT\"]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15a17d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating + Evaluating: 100%|██████████| 125/125 [00:36<00:00,  3.44it/s, ALL_EM=0.5124, Long_Tail=0.3571, Infrequent=0.5814, Frequent=0.2857]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved report: results/meta-llama_Meta-Llama-3-8B-FUSEQA_20260220-1050.report.txt | Time=36.32s | ALL=0.5200 | LONG-TAIL=0.3448 | INFREQUENT=0.5955 | FREQUENT=0.2857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'results/meta-llama_Meta-Llama-3-8B-FUSEQA_20260220-1050.report.txt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "TOP_K = 3\n",
    "\n",
    "RESULTS_DIR = \"results\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "file_name = hf_model_to_filename(MODEL_NAME + \"-\" + RUN_TYPE)\n",
    "outfile   = os.path.join(RESULTS_DIR, file_name + \".jsonl\")\n",
    "\n",
    "model_device = next(model.parameters()).device\n",
    "WRITE_OUTPUTS = True  # or True if you want JSONL output\n",
    "\n",
    "with (open(outfile, \"w\", encoding=\"utf-8\", buffering=1) if WRITE_OUTPUTS else nullcontext()) as writer:\n",
    "\n",
    "    pbar = tqdm.tqdm(total=len(ds), desc=\"Generating + Evaluating\", dynamic_ncols=True)\n",
    "\n",
    "    for i in range(len(ds)):\n",
    "\n",
    "        ex = {k: ds[k][i] for k in ds.column_names}\n",
    "\n",
    "        q, s_pop = ex[\"question\"], int(ex.get(\"s_pop\", 0))\n",
    "        tier = tier_from_spop(s_pop)\n",
    "\n",
    "        gold = parse_list(ex.get(\"possible_answers\"))\n",
    "        gold_norm_set = {norm(g) for g in gold if norm(g)}\n",
    "\n",
    "        retrieved = ex.get(\"retrieved_docs\") or []\n",
    "        context = build_context(retrieved, k=TOP_K) if USE_CONTEXT else \"\"\n",
    "\n",
    "        pred = ask_llm_generate(model=model, tokenizer=tokenizer, question=q, context=context, use_context=USE_CONTEXT, device=model_device)\n",
    "\n",
    "        pred_norm = norm(pred)\n",
    "        em = int(pred_norm in gold_norm_set) if gold_norm_set else 0\n",
    "\n",
    "        update_metrics(tier, em)\n",
    "\n",
    "        if WRITE_OUTPUTS:\n",
    "            record = {\"i\": i, \"s_pop\": s_pop, \"tier\": tier, \"question\": q, \"gold\": gold, \"pred\": pred, \"em\": em}\n",
    "            write_record(writer, record)\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            pbar.set_postfix({k: f\"{v:.4f}\" for k, v in current_scores().items()})\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "generate_report(counts, em_hits, file_name, model_name=MODEL_NAME, run_type=RUN_TYPE, total_time=total_time)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
