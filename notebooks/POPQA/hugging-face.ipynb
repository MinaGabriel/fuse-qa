{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0fd834",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /content/fuse-qa\n",
    "# Clone repo\n",
    "!git clone https://github.com/MinaGabriel/fuse-qa.git\n",
    "!export HF_HUB_ENABLE_HF_TRANSFER=1\n",
    "%cd /content/fuse-qa\n",
    "!pip install -q -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a8cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import logging\n",
    "from huggingface_hub import login\n",
    "\n",
    "from fuseqa import *\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Setup\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B\"\n",
    "# MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "# MODEL_NAME = \"google/gemma-2-2b-it\"\n",
    "# MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "MODEL_NAME = \"openai/gpt-oss-20b\"\n",
    "RUN_TYPE = \"FUSEQA\"\n",
    "USE_CONTEXT = RUN_TYPE in (\"FUSEQA\", \"FUSEQA-SRE\")\n",
    "\n",
    "print(hf_model_to_filename(MODEL_NAME + \"-\" + RUN_TYPE))\n",
    "print(\"GPUs:\", torch.cuda.device_count())\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# HuggingFace Auth (optional)\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "if (token := os.getenv(\"HF_TOKEN\")):\n",
    "    login(token=token)\n",
    "    print(\"HF login: done\")\n",
    "else:\n",
    "    print(\"HF login: skipped\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Load Model + Tokenizer\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Device Info\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "print(\"Model device:\", device)\n",
    "\n",
    "if hasattr(model, \"hf_device_map\"):\n",
    "    print(\"Device map:\", model.hf_device_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070af6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"MinaGabriel/popqa-with-retrieval-20\")[\"test\"].select(range(125))\n",
    "#ds = load_dataset(\"MinaGabriel/popqa-with-retrieval-20\")[\"test\"]\n",
    "len(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8b3e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REPORTS:\n",
    "import os\n",
    "import tqdm\n",
    "from contextlib import nullcontext\n",
    "\n",
    "counts  = {g: 0 for g in (\"ALL\", \"LONG-TAIL\", \"INFREQUENT\", \"FREQUENT\")}\n",
    "em_hits = {g: 0 for g in (\"ALL\", \"LONG-TAIL\", \"INFREQUENT\", \"FREQUENT\")}\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "def update_metrics(tier, em):\n",
    "    counts[\"ALL\"] += 1\n",
    "    em_hits[\"ALL\"] += em\n",
    "\n",
    "    if tier in counts:\n",
    "        counts[tier] += 1\n",
    "        em_hits[tier] += em\n",
    "\n",
    "\n",
    "def current_scores():\n",
    "    return {\n",
    "        \"ALL_EM\":     safe_div(em_hits[\"ALL\"],        counts[\"ALL\"]),\n",
    "        \"Long_Tail\":  safe_div(em_hits[\"LONG-TAIL\"],  counts[\"LONG-TAIL\"]),\n",
    "        \"Infrequent\": safe_div(em_hits[\"INFREQUENT\"], counts[\"INFREQUENT\"]),\n",
    "        \"Frequent\":   safe_div(em_hits[\"FREQUENT\"],   counts[\"FREQUENT\"]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c29702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PromptConfig:\n",
    "    system_no_context: str = (\n",
    "    \"You are a precise factual question answering system.\\n\"\n",
    "    \"Return only the exact answer span.\"\n",
    "    )\n",
    "\n",
    "    system_with_context: str = (\n",
    "        \"You are a strict answer extraction system.\\n\"\n",
    "        \"Extract the exact answer span from the context.\"\n",
    "    )\n",
    "\n",
    "    rules: Tuple[str, ...] = (\n",
    "    \"Answer with the shortest possible span (1-3 words).\",\n",
    "    \"Do not explain.\",\n",
    "    \"Do not explain. Do not repeat the question.\",\n",
    ")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a17d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TOP_K = 3\n",
    "\n",
    "RESULTS_DIR = \"results\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "file_name = hf_model_to_filename(MODEL_NAME + \"-\" + RUN_TYPE)\n",
    "outfile   = os.path.join(RESULTS_DIR, file_name + \".jsonl\")\n",
    "\n",
    "model_device = next(model.parameters()).device\n",
    "WRITE_OUTPUTS = True  # or True if you want JSONL output\n",
    "\n",
    "with (open(outfile, \"w\", encoding=\"utf-8\", buffering=1) if WRITE_OUTPUTS else nullcontext()) as writer:\n",
    "\n",
    "    pbar = tqdm.tqdm(total=len(ds), desc=\"Generating + Evaluating\", dynamic_ncols=True)\n",
    "\n",
    "    for i in range(len(ds)):\n",
    "\n",
    "        ex = {k: ds[k][i] for k in ds.column_names}\n",
    "\n",
    "        q, s_pop = ex[\"question\"], int(ex.get(\"s_pop\", 0))\n",
    "        tier = tier_from_spop(s_pop)\n",
    "\n",
    "        gold = parse_list(ex.get(\"possible_answers\"))\n",
    "        gold_norm_set = {norm(g) for g in gold if norm(g)}\n",
    "\n",
    "        retrieved = ex.get(\"retrieved_docs\") or []\n",
    "        context = build_context(retrieved, k=TOP_K) if USE_CONTEXT else \"\"\n",
    "\n",
    "        pred = ask_llm_generate(prompt_cfg = PromptConfig(), model=model, tokenizer=tokenizer, question=q, context=context, use_context=USE_CONTEXT, device=model_device, print_prompt=True)\n",
    "\n",
    "        pred_norm = norm(pred)\n",
    "        em = int(pred_norm in gold_norm_set) if gold_norm_set else 0\n",
    "\n",
    "        update_metrics(tier, em)\n",
    "\n",
    "        if WRITE_OUTPUTS:\n",
    "            record = {\"i\": i, \"s_pop\": s_pop, \"tier\": tier, \"question\": q, \"gold\": gold, \"pred\": pred, \"em\": em}\n",
    "            write_record(writer, record)\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            pbar.set_postfix({k: f\"{v:.4f}\" for k, v in current_scores().items()})\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "generate_report(counts, em_hits, file_name, model_name=MODEL_NAME, run_type=RUN_TYPE, total_time=total_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e726b711",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -lh results/*.jsonl\n",
    "! cat results/*.jsonl | head -3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
