{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae0fd834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'fuse-qa' already exists and is not an empty directory.\n",
      "/content/fuse-qa\n",
      "Obtaining file:///content/fuse-qa\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: fuseqa\n",
      "  Building editable for fuseqa (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for fuseqa: filename=fuseqa-0.1.0-0.editable-py3-none-any.whl size=3454 sha256=f6098dece2098a6d84611772a4bfd9d31d0d54f596a6ce72b363fbb95b57ce82\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-l9usavu4/wheels/a8/5d/ed/eaf3d3d6bc81274f10724114d2f643b34283e350e2beed7b2d\n",
      "Successfully built fuseqa\n",
      "Installing collected packages: fuseqa\n",
      "  Attempting uninstall: fuseqa\n",
      "    Found existing installation: fuseqa 0.1.0\n",
      "    Uninstalling fuseqa-0.1.0:\n",
      "      Successfully uninstalled fuseqa-0.1.0\n",
      "Successfully installed fuseqa-0.1.0\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/MinaGabriel/fuse-qa.git\n",
    "%cd /content/fuse-qa\n",
    "! export HF_TOKEN=\n",
    "! pip install -e . \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37a8cb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai_gpt-oss-20b-FUSEQA_20260220-1626\n",
      "GPUs: 1\n",
      "HF login: skipped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f3e452232e4d02b7d94999ff3fe86e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/411 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import logging\n",
    "from huggingface_hub import login\n",
    "\n",
    "from fuseqa import *\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Setup\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B\"\n",
    "# MODEL_NAME = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "# MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "# MODEL_NAME = \"google/gemma-2-2b-it\"\n",
    "# MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "MODEL_NAME = \"openai/gpt-oss-20b\"\n",
    "RUN_TYPE = \"FUSEQA\"\n",
    "USE_CONTEXT = RUN_TYPE in (\"FUSEQA\", \"FUSEQA-SRE\")\n",
    "\n",
    "print(hf_model_to_filename(MODEL_NAME + \"-\" + RUN_TYPE))\n",
    "print(\"GPUs:\", torch.cuda.device_count())\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# HuggingFace Auth (optional)\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "if (token := os.getenv(\"HF_TOKEN\")):\n",
    "    login(token=token)\n",
    "    print(\"HF login: done\")\n",
    "else:\n",
    "    print(\"HF login: skipped\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Load Model + Tokenizer\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# Device Info\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "print(\"Model device:\", device)\n",
    "\n",
    "if hasattr(model, \"hf_device_map\"):\n",
    "    print(\"Device map:\", model.hf_device_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "070af6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"MinaGabriel/popqa-with-retrieval-20\")[\"test\"].select(range(125))\n",
    "#ds = load_dataset(\"MinaGabriel/popqa-with-retrieval-20\")[\"test\"]\n",
    "len(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d8b3e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#REPORTS:\n",
    "import os\n",
    "import tqdm\n",
    "from contextlib import nullcontext\n",
    "\n",
    "counts  = {g: 0 for g in (\"ALL\", \"LONG-TAIL\", \"INFREQUENT\", \"FREQUENT\")}\n",
    "em_hits = {g: 0 for g in (\"ALL\", \"LONG-TAIL\", \"INFREQUENT\", \"FREQUENT\")}\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "def update_metrics(tier, em):\n",
    "    counts[\"ALL\"] += 1\n",
    "    em_hits[\"ALL\"] += em\n",
    "\n",
    "    if tier in counts:\n",
    "        counts[tier] += 1\n",
    "        em_hits[tier] += em\n",
    "\n",
    "\n",
    "def current_scores():\n",
    "    return {\n",
    "        \"ALL_EM\":     safe_div(em_hits[\"ALL\"],        counts[\"ALL\"]),\n",
    "        \"Long_Tail\":  safe_div(em_hits[\"LONG-TAIL\"],  counts[\"LONG-TAIL\"]),\n",
    "        \"Infrequent\": safe_div(em_hits[\"INFREQUENT\"], counts[\"INFREQUENT\"]),\n",
    "        \"Frequent\":   safe_div(em_hits[\"FREQUENT\"],   counts[\"FREQUENT\"]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2c29702",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PromptConfig:\n",
    "    system_no_context: str = (\n",
    "    \"You are a precise factual question answering system.\\n\"\n",
    "    \"Return only the exact answer span.\"\n",
    "    )\n",
    "\n",
    "    system_with_context: str = (\n",
    "        \"You are a strict answer extraction system.\\n\"\n",
    "        \"Extract the exact answer span from the context.\"\n",
    "    )\n",
    "\n",
    "    rules: Tuple[str, ...] = (\n",
    "    \"Answer with the shortest possible span (1-3 words).\",\n",
    "    \"Do not explain.\",\n",
    "    \"Do not explain. Do not repeat the question.\",\n",
    ")   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15a17d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating + Evaluating:   0%|          | 0/125 [00:27<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ask_llm_generate() got an unexpected keyword argument 'prompt_cfg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-4014146403.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrieved\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTOP_K\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mUSE_CONTEXT\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mask_llm_generate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPromptConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mUSE_CONTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mpred_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: ask_llm_generate() got an unexpected keyword argument 'prompt_cfg'"
     ]
    }
   ],
   "source": [
    "\n",
    "TOP_K = 3\n",
    "\n",
    "RESULTS_DIR = \"results\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "file_name = hf_model_to_filename(MODEL_NAME + \"-\" + RUN_TYPE)\n",
    "outfile   = os.path.join(RESULTS_DIR, file_name + \".jsonl\")\n",
    "\n",
    "model_device = next(model.parameters()).device\n",
    "WRITE_OUTPUTS = True  # or True if you want JSONL output\n",
    "\n",
    "with (open(outfile, \"w\", encoding=\"utf-8\", buffering=1) if WRITE_OUTPUTS else nullcontext()) as writer:\n",
    "\n",
    "    pbar = tqdm.tqdm(total=len(ds), desc=\"Generating + Evaluating\", dynamic_ncols=True)\n",
    "\n",
    "    for i in range(len(ds)):\n",
    "\n",
    "        ex = {k: ds[k][i] for k in ds.column_names}\n",
    "\n",
    "        q, s_pop = ex[\"question\"], int(ex.get(\"s_pop\", 0))\n",
    "        tier = tier_from_spop(s_pop)\n",
    "\n",
    "        gold = parse_list(ex.get(\"possible_answers\"))\n",
    "        gold_norm_set = {norm(g) for g in gold if norm(g)}\n",
    "\n",
    "        retrieved = ex.get(\"retrieved_docs\") or []\n",
    "        context = build_context(retrieved, k=TOP_K) if USE_CONTEXT else \"\"\n",
    "\n",
    "        pred = ask_llm_generate(prompt_cfg = PromptConfig(), model=model, tokenizer=tokenizer, question=q, context=context, use_context=USE_CONTEXT, device=model_device)\n",
    "\n",
    "        pred_norm = norm(pred)\n",
    "        em = int(pred_norm in gold_norm_set) if gold_norm_set else 0\n",
    "\n",
    "        update_metrics(tier, em)\n",
    "\n",
    "        if WRITE_OUTPUTS:\n",
    "            record = {\"i\": i, \"s_pop\": s_pop, \"tier\": tier, \"question\": q, \"gold\": gold, \"pred\": pred, \"em\": em}\n",
    "            write_record(writer, record)\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            pbar.set_postfix({k: f\"{v:.4f}\" for k, v in current_scores().items()})\n",
    "\n",
    "    pbar.close()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "\n",
    "generate_report(counts, em_hits, file_name, model_name=MODEL_NAME, run_type=RUN_TYPE, total_time=total_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e726b711",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -lh results/*.jsonl\n",
    "! cat results/*.jsonl | head -3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
